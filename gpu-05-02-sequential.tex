\subsubsection{Achieving an Efficient Baseline Sequential Version}

This subsection presents the results of transforming the algorithm implementation
from Python to C++, and then removing obvious inefficiencies, to achieve a
good baseline implementation for comparison with the parallel implementation.
%
We also investigate the performance impact of using
32-bit floating point numbers in stead of 64-bit doubles.
This hardly reduces the computation intensity for the Xeon, but it very much
affects data intensity because the data sizes are halved.
%
% It has been previously shown that stochastic learning
% algorithms do not require high precision in the presence
% of statistical approximations and the addition of random
% noise [?].

Due to the performance limitations of the Python implementation, the
experimental configuration of this section is minimal.
%
Namely, we use the CA-HepPh dataset; the number of
iterations is 1000.
%Note that these trial runs stop very far before convergence; using a small
%number of
%iterations is valid because the time spent in an iteration does not vary
%during the lifetime of the algorithm.
The selected number of communities is~1024, the mini-batch size~32, the
neighbor sample size~32.

The mini-batch sampling as described in Section~\ref{sec-background} randomly
chooses either a mini-batch of link edges, whose size is the degree of one
randomly selected vertex, or a mini-batch of nonlink edges, whose size is
specified as a model parameter. In this evaluation, we only select batches
of nonlink edges because that makes the mini-batch size, and hence the
execution times, deterministic. We separately validated that the time spent
\textit{per mini-batch vertex} for samples of link edges and nonlink edges
is fully consistent.

\begin{table}[b]
\center\begin{tabular}{l d{5.1} d{2.1} d{2.1} d{2.1}}
	&	& \multicolumn{3}{c}{C++} \\
\cline{3-5}
	& & 
            \multicolumn{1}{c}{Python} &
                \multicolumn{1}{c}{float64} &
		    \multicolumn{1}{c}{float32} \\
\multicolumn{1}{l}{Algorithm stage} &\multicolumn{1}{c}{Python} &
            \multicolumn{1}{c}{idiom} &
                \multicolumn{1}{c}{(baseline)} &
		    \multicolumn{1}{c}{baseline} \\
\hline
sample mini-batch       &      0.03 &  0.03 &  0.03 & 0.02 \\
sample neighbor sets    &      8.5  &  0.5  &  0.4  & 0.35 \\
update\_phi             &  9,014    & 52.7  &  8.9  & 4.9  \\
update\_pi              &     93.1  & 40.3  &  0.11 & 0.05 \\
update\_theta\_beta     &    339    &  2.1  &  0.76 & 0.55 \\
perplexity*             &    178    &  0.18 &  0.18 & 0.26 \\
\hline
\\[-1ex]
\end{tabular}
\caption{Performance comparison between Python, C++ that follows the Python
idioms, and our baseline C++ implementations. 1000 iterations, times in
\textrm{ms} per iteration; *perplexity time divided by 100 iterations.}
\label{table-python-comparison}
\end{table}

Table~\ref{table-python-comparison} compares performance between Python,
the C++ version labeled 'Python Idiom' with the inefficiencies inherited from
Python, then our baseline C++ version which has the inefficiencies removed,
and finally the same with 32-bit floats.
%
We present timings for the compute stages of the algorithm described in
Section~\ref{sec-background}, with the exception of
\textit{update\_theta\_beta} which combines \textit{update\_theta}
and \textit{update\_beta}.
%
The numbers are times in ms. For all stages except perplexity, the times
are per iteration.
%
Once every so many iterations (hundreds or thousands in production runs),
the perplexity calculation is invoked. The perplexity time in the table
has been divided by 100 iterations; per perplexity invocation, it is large in
comparison with the iteration stage times, but it is amortized over those
iterations.

In all implementations, \textit{update\_phi} dominates the computation.
The translation from Python into 'Python Idiom' C++ speeds up this stage
by a factor~171, even though the Python implementation uses numpy for its data
structures. Comparison of the C++ table entries for 'Python idiom' and
64-bit float baseline shows that our C++ optimizations speed up this stage
by another factor of~6. Reducing the floating-point precision to 32~bits
doubles this factor, which shows that the algorithm is very much data-dominant.
%
\textit{update\_pi} has a disproportionately large speed
gain after removing inefficiencies. This was attained by limiting the
update to $\pi$ to only those values of $\pi$ that have changed in this
iteration, in stead of the full $\pi$ array (which, incidentally, is handled
rather efficiently by numpy).
\textit{update\_theta\_beta} gave fewer opportunities for C++ optimization,
in the sampling stages there was none. In \textit{mini-batch-sampling} there
is no speedup compared to Python because it consists of a call to
a fast Python random primitive that cannot be bettered from C++.
The total performance gain between Python and the 64-bit float baseline C++
implementation exceeds a factor of~1000, and this grows to a factor of over~1500
if the floating-point precision is reduced to 32~bits. The 32-bit precision
implementation will be the baseline for the parallel performance comparison.

\begin{comment}
The introduction of a custom user-space random generator brings at most
a very small
benefit. We show it, because it is necessary for the multi-threaded
implementations described in the next section, and this measurement serves to
prove that it does not harm execution speed.
\end{comment}

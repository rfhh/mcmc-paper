\section{System Design}
\label{design-section}

\begin{comment}
- distributed parallelisation:
  cut up into stages like the CC-Grid paper:
  pipeline:
    1) sample mini-batch
    2) sample neighbors
    3) update_phi
  4) update_pi
  5) update_theta
  5a) calc_grads
  5b) sum_grads
  5c) update_theta
  6) update_beta
  7) perplexity

- DKV RDMA store

- data dependencies:
  mb = nodes in the mini-batch
  nb = nodes in the neighbor sets
  H  = held-out set

  	pi	phi	beta	theta	grad.th	G	prev.mini-batch
  1) 	-	-	-	-		R	-
  2) 	-	-	-	-		R	R
  3) 	R(mb)	R	R	-		R	-
	R(nb)
  4)	R/W(mb)	R	-	-		-	-
  5a)	R(mb)	-	R	R	W	-	-
  5b)	-	-	-	-	R/W	-	-
  5c)	-	-	-	R/W	R	-	-
  6)	-	-	W	R	-	-	-
  7)	R(H)	-	R(H)	-	-	H	-

- parallelization:
  1) sequential at master's, multithreaded
  2) parallel over mb; at workers', multithreaded
  3) parallel over mb; at workers', multithreaded
  4) parallel over mb; at workers', multithreaded
  5a) parallel at workers', multithreaded
  5b) omp_reduce(+) at workers', MPI reduce global
  5c) at master's, multithreaded
  6) broadcast theta by master, parallel update at workers'
  7) parallel over H; at workers', multithreaded
     reduce(+) at workers, MPI reduce global

- distributed parallelization:
  trade memory size for communication costs
  pity: the algorithm is already memory-dominant

- synchronization:
  in principle, each step synchronized
  but: pipelining in sample mini-batch / load pi/phi / update_phi

- data sizes:
  G has N vertices and |E| undirected edges
  |H| is 1..10% of |E|
  pi, phi, beta, theta are float32
  pi[N][K + 1] if phi folded into pi
  beta[K]
  theta[K][2]
  so, pi is the memory eater
   . beta/theta are replicated
   . phi is folded into pi
   . pi is partitioned across the workers
  G is stored at the master only: save the memory for pi so we can handle
  larger K. The relevant slices of G (m's adjacency lists) are scattered
  together with the mini-batch.
  G is stored as a Google SparseHash because c++ std::(unordered_)set has a
  huge overhead per item. Workers unpack their slice of G into a
  std::unordered_set because that is somewhat faster.

- pi storage: DKV
   . pi/phi is read in 3), 4), 5a), 7)
   . pi is written in 4); read[i], then write[i], so no other data dependencies
   . so access is very synchronous: either read-only, or write without
     concurrent readers
   . DKV store properties:
      - single-sized keys
      - contiguous key space (integers 0..N-1)
      - update-only writes: no new KVs, no deletes
     so: no load imbalance, no hashing that is worth its name
      - no read/write or write/write concurrency
     so:
      - build RDMA store with remote reads and remote writes only, with exactly
        one RDMA transaction per read or write
\end{comment}

% rationale -- here or in the intro?

% http://stanford.edu/~rezab/nips2014workshop/slides/jure.pdf

The SG-MCMC algorithm described in the previous section has an abundance of
opportunities for parallelism, for the multi-threaded shared-memory type as
well as for distributed-memory parallelism. The benefit of multi-threaded
parallelism is the speedup of the computation. Additionally, a distributed implementation
allows the storage of data in the collective memory of a cluster
and increases aggregate memory bandwidth. Both of these parameters scale with the number of
machines. The downside of a distributed implementation is that it requires
considerably more programming effort.

% intro parallelisation, distribution

This section describes how we parallelized the algorithm, which had been
implemented sequentially in~C++.
The usage of multi-threaded parallelism
is straightforward, therefore, we will discuss its details only where appropriate. The
distributed design follows a master-worker paradigm, where
the master controls the sequence of parallel operations and the workers perform the
calculations. For thread parallelism, we annotate the program with
OpenMP~\cite{OpenMPSpec}. For distributed communication, we use
MPI~\cite{Forum:1994:MMI:898758}.

% largest data structures

\subsection{Data distribution}


The largest data structures of the algorithm are \Edges, $\cE_h$, $\pi$
and~$\phi$, see Table~\ref{tbl-symbols}.
For the largest dataset in this paper, com-Friendster, \Edges has 1.8~billion
undirected edges. In our representation with directed edges, this takes
up 13.5GB. Our design lets \Edges reside only at the master's. We observe that
the calculations in the update stages only require the subset of \Edges that
is touched by the mini-batch vertices, so the master scatters that subset to
the workers together with the scattering of the mini-batch vertices. This way,
we trade a reduction in memory usage at the workers against limited
communication costs. In contrast, $\cE_h$ is replicated at
all machines for the neighbor sampling and the parallel perplexity calculation.

%\looseness=-1
$\pi$~and $\phi$ are 32-bit float arrays of size $K \times N$. For our
largest distributed experiment, com-Friendster with $N$=64M and $K$=12K, each
requires~3TB. We decided to store only $\pi$ and $\sum\phi$, and recalculate
$\phi$ from these whenever appropriate. Here, we trade a substantial gain
in memory usage against some computation. $\pi$ is partitioned across the workers,
and $\pi$ values are accessed via a DKV (distributed key-value) store.

% lack of locality

The distribution within the graph of the vertices of the mini-batch as well as
the neighbor sets is completely random. That means that there is \textit{no
locality}
in $\pi$ access patterns, especially in the accesses to the neighbor sets
$\Neighbors$.
Hence, there is no opportunity to exploit data locality through caching of
$\pi$.

\subsection{RDMA Distributed Key-Value Store}

%\looseness=-1
Modern network technology allows for Remote Direct Memory Access (RDMA)
through the network cards (NICs) with negligible
overhead~\cite{Hamada_infinibandtrade,Beck:2011:PER:2043535.2043537}. The use
of RDMA enables communicating peers to avoid the high latency for
interrupting a processor and waiting for it to act upon a request. Instead, a
process registers a region of memory that can be directly accessed by the
NIC. % RDMA only provides primitive operations and limited abstractions.
This has opened up the development of
efficient remote memory services, often in the form of distributed key-value
(DKV) stores.

The distributed algorithm implementation stores $\pi$ in a DKV store in the
collective memory of the
cluster, where $\pi[i]+\sum\phi[i]$ is the value for key $i$.
%
We decided not
to use any of the existing implementations of RDMA DKV stores, and instead
build our own DKV store on top of the Infiniband ib-verbs~API.
%
This is motivated by the fact that our use case is unique in a number
of important aspects. First, the number of keys and values to be stored are
constant throughout the execution of the algorithm. For instance, no records
are dynamically inserted or deleted from the DKV after their initial insertion,
although the value associated with each key can be updated. Additionally, all
keys are of the same type and represent a unique vertex in the graph.
Therefore, keys can be represented as integer identifiers. Moreover, each key
points to a vector of $K+1$ floats as its value.
%
This data representation can be exploited to simplify the management of
read and write requests. Similarly, the keys can be deterministically
partitioned across the cluster using a fast hash function.

The algorithm's access pattern is well controlled because the
computation is partitioned into stages that are separated by barriers. A
single stage either reads values or updates values. The algorithm ensures that updates
are always targeting unique elements,
therefore, there are no read/write or write/write hazards. The existing DKV
stores (RamCloud~\cite{Ousterhout:2015:RSS:2818727.2806887}, FaRM~\cite{179767},
and Herd~\cite{Kalia:2014:URE:2740070.2626299}) incur
overhead for insert/delete flexibility, concurrency control, or variable-sized
values. Our use case allows us to do any operation in exactly one RDMA read
or RDMA write.
%
Herd argues that replacing RDMA reads by
an RPC to the server, followed by an unreliable write, is faster.
%
However,
the paper shows this only holds for small payload packets, up to 256B. A $\pi$
packet in our application is typically thousands to hundreds of thousands
of 4-byte floats.

% horrible hack to move the table up
\begin{table*}
  \centering
  \begin{tabular}{l r r r l}
    Name            & \#Vertices &       \#Edges & \#Ground-truth & Description \\
                    &            &               & communities    &             \\
    \hline
    com-LiveJournal &  3,997,962 &    34,681,189 & 287,512        & Online blogging social network \\
    com-Friendster  & 65,608,366 & 1,806,067,135 & 957,154        & Online gaming social network \\
    com-Orkut       &  3,072,441 &   117,185,083 & 6,288,363      & Online social network \\
    com-Youtube     &  1,134,890 &     2,987,624 & 8,385          & Video-sharing social network \\
    com-DBLP        &    317,080 &     1,049,866 & 13,477         & Computer science bibliography collaboration network \\
    com-Amazon      &    334,863 &       925,872 & 75,149         & Product co-purchasing network \\
    \hline
  \end{tabular}
  \caption{Summary of SNAP graph data sets used for evaluation.}
  \label{tab-datasets}
\end{table*}

% each of the stages

\subsection{Implementation of distributed parallelism}

This section describes the parallelization of each of the stages of the
algorithm's main loop. All but the first stage justify parallelization for
high values of combinations of the parameters mini-batch size $|\Minibatch|$,
number of vertices in the mini-batch $M$,
number of communities $K$, neighbor sample size $|\Neighbors|$, and held-out set
size $|\cE_h|$.

The first stage, mini-batch selection (line 3 in Algorithm~\ref{algorithm}),
is done by the master and it is
not itself parallelized. However, the distributed implementation overlaps
its execution using pipeline parallelism while the workers are
performing \textit{update\_phi}. The mini-batch is partitioned equally over
the workers and the relevant sections of \Edges are distributed together with the
mini-batch subsets.

After a worker has received its subset of the mini-batch, it samples a neighbor
set $\Neighbors$ for each of its mini-batch vertices, using thread parallelism
(line 5 in Algorithm~\ref{algorithm}).

The next stage, \textit{update\_phi} (line 6 in Algorithm~\ref{algorithm}), is the algorithm's dominant stage, not
only in calculation but also in memory accesses. It performs $M \times
|\Neighbors| \times
K$ operations. It can be fully parallelized because it is a data-parallel
operation over each of the mini-batch vertices. \textit{update\_phi} loads the
$\pi$ values for the local mini-batch vertices and their neighbors from the
DKV store. The updates to $\phi$ for the mini-batch vertices are calculated
independently using thread parallelism.

%\looseness=-1
These updates are used by \textit{update\_pi} (line 7 in
Algorithm~\ref{algorithm}) locally, besides the value of
$\pi/\phi$ for the mini-batch vertices. The update to $\pi$ requires
$M \times K$ operations, and it is done in parallel over mini-batch vertices. Because of
memory consistency, this stage awaits completion of \textit{update\_phi} with
an MPI barrier. After the calculation, the updated values of $\pi+\sum\phi$
are written to the DKV store.

\textit{update\_beta} (lines 10 and 11 in Algorithm~\ref{algorithm}) requires $|\Minibatch| \times K$ operations. It is preceded by an MPI
barrier to ensure that up-to-date $\pi$ values are read. \textit{update\_beta}
is split into four steps. The first stage partitions the mini-batch across
machines to calculate contributions to $g_{ab}(\theta)$. The values of $\pi$ for
the local mini-batch vertices are loaded from the DKV store. These calculations
are done with thread parallelism over the mini-batch vertices. In the second step,
a multi-threaded summation is performed to calculate the contributions per
machine, followed by a distributed summation using an MPI reduce operation. The
third step calculates $\beta$ from $\theta$ sequentially at the master. This
operation takes only $K$ steps. The resulting $\beta$ is broadcast with MPI
to the workers.

The calculation of perplexity requires $|\cE_h|\times K$ steps. Each of the
machines owns a subset of $\cE_h$. It loads up-to-date values of $\pi$ for each
vertex in its part of $\cE_h$, then for each of its edges in $\cE_h$ it calculates the
contribution to the perplexity using thread parallelism. The resulting
contributions are summed to the global perplexity value in two stages,
with a local OpenMP reduction followed by a global MPI reduction.

Note that the calculation requirements per iteration do not depend
on $N$ or $|\cE^*|$, the size of the network graph. However, larger graphs are
expected to require more iterations to achieve convergence.

\subsection{Pipelining of computation and loading $\pi$}

The distributed design features two instances of pipeline parallelism. First,
at the process level, sampling of the minibatch by the master is decoupled
from \textit{update\_phi} by the workers, so the master computes the minibatch
for the next iteration while the workers compute \textit{update\_phi} for the
current iteration. Second, at the threads level within the workers, loading of
$\pi$ and \textit{update\_phi} are split up into chunks, and the calculation
in \textit{update\_phi} for the current chunk is done simultaneously with
loading $\pi$ for the next chunk. The rationale for implementing pipeline
parallelism in these places of the program, is that loading $\pi$ is the
dominant contribution within the dominant stage \textit{update\_phi}, as
will be shown in Section~\ref{eval-pipeline}.  Since loading $\pi$ is the
performance bottleneck of the distributed algorithm, performing other work
in parallel shortens the critical path.

% \subsection{System Design}
\label{design-section}

\begin{comment}
- distributed parallelization:
  cut up into stages like the CC-Grid paper:
  pipeline:
    1) sample mini-batch
    2) sample neighbors
    3) update_phi
  4) update_pi
  5) update_theta
  5a) calc_grads
  5b) sum_grads
  5c) update_theta
  6) update_beta
  7) perplexity

- DKV RDMA store

- data dependencies:
  mb = nodes in the mini-batch
  nb = nodes in the neighbor sets
  H  = held-out set

  	pi	phi	beta	theta	grad.th	G	prev.mini-batch
  1) 	-	-	-	-		R	-
  2) 	-	-	-	-		R	R
  3) 	R(mb)	R	R	-		R	-
	R(nb)
  4)	R/W(mb)	R	-	-		-	-
  5a)	R(mb)	-	R	R	W	-	-
  5b)	-	-	-	-	R/W	-	-
  5c)	-	-	-	R/W	R	-	-
  6)	-	-	W	R	-	-	-
  7)	R(H)	-	R(H)	-	-	H	-

- parallelization:
  1) sequential at master's, multithreaded
  2) parallel over mb; at workers', multithreaded
  3) parallel over mb; at workers', multithreaded
  4) parallel over mb; at workers', multithreaded
  5a) parallel at workers', multithreaded
  5b) omp_reduce(+) at workers', MPI reduce global
  5c) at master's, multithreaded
  6) broadcast theta by master, parallel update at workers'
  7) parallel over H; at workers', multithreaded
     reduce(+) at workers, MPI reduce global

- distributed parallelization:
  trade memory size for communication costs
  pity: the algorithm is already memory-dominant

- synchronization:
  in principle, each step synchronized
  but: pipelining in sample mini-batch / load pi/phi / update_phi

- data sizes:
  G has N vertices and |E| undirected edges
  |H| is 1..10% of |E|
  pi, phi, beta, theta are float32
  pi[N][K + 1] if phi folded into pi
  beta[K]
  theta[K][2]
  so, pi is the memory eater
   . beta/theta are replicated
   . phi is folded into pi
   . pi is partitioned across the workers
  G is stored at the master only: save the memory for pi so we can handle
  larger K. The relevant slices of G (m's adjacency lists) are scattered
  together with the mini-batch.
  G is stored as a Google SparseHash because c++ std::(unordered_)set has a
  huge overhead per item. Workers unpack their slice of G into a
  std::unordered_set because that is somewhat faster.

- pi storage: DKV
   . pi/phi is read in 3), 4), 5a), 7)
   . pi is written in 4); read[i], then write[i], so no other data dependencies
   . so access is very synchronous: either read-only, or write without
     concurrent readers
   . DKV store properties:
      - single-sized keys
      - contiguous key space (integers 0..N-1)
      - update-only writes: no new KVs, no deletes
     so: no load imbalance, no hashing that is worth its name
      - no read/write or write/write concurrency
     so:
      - build RDMA store with remote reads and remote writes only, with exactly
        one RDMA transaction per read or write
\end{comment}

% rationale -- here or in the intro?

% http://stanford.edu/~rezab/nips2014workshop/slides/jure.pdf

The implementations in the previous section put a limit to the size of
community networks and number of communities that can be processed: the
state must fit into GPU memory. To tackle networks of much larger size, we
created a distributed implementation that stores the data in the
collective memory of a cluster. The downside of a distributed implementation
is that the burden of memory access is increased for our already data-dominant
algorithm.

% intro parallelisation, distribution

This section describes how we created a distributed version of the C++
algorithm. We used the multicore CPUs in the DAS5 with straightforward
intra-node thread parallelisation, not the GPU implementations described
in the previous section. The justification is twofold: in our DAS5 system,
it is pointless to speed up the intra-node performance beyond straightforward
multithreading, because distributed performance is dominated by remote memory
latency; the analysis in Section~\ref{eval-pipeline} shows that the time for
(pre)loading the data from remote hosts greatly exceeds the calculation time.
The calculation time is fully overlapped in our pipelining implementation.
Moreover, usage of GPUs in the distributed setting would necessitate staging of
the relevant parts of $\pi/\phi$ from the host to GPU memory in each iteration,
which would eat most of the performance gain offered by the GPU because
the algorithm is so data-intensive.  % The details of the multi-threaded
parallelism % will be discussed only where appropriate.  The distributed design
follows a master-worker paradigm, where the master controls the sequence of
parallel operations and the workers perform the calculations. For thread
parallelism, we annotate the program with OpenMP~\cite{OpenMPSpec}. For
distributed communication, we use MPI~\cite{Forum:1994:MMI:898758}.

% largest data structures

\subsection{Data distribution}


The largest data structures of the algorithm are \Edges, $\cE_h$, $\pi$
and~$\phi$, see Table~\ref{tbl-symbols}.
For the largest data set in this paper, com-Friendster, \Edges has 1.8~billion
undirected edges. In our representation with directed edges, this takes
up 13.5GB. Our design lets \Edges reside only at the master's. We observe that
the calculations in the update stages only require the subset of \Edges that
is touched by the mini-batch vertices, so the master scatters that subset to
the workers together with the scattering of the mini-batch vertices. This way,
we trade a reduction in memory usage at the workers against limited
communication costs. In contrast, $\cE_h$ is replicated at
all machines for the neighbor sampling and the parallel perplexity calculation.

%\looseness=-1
$\pi$~and $\phi$ are 32-bit float arrays of size $K \times N$. For our
largest distributed experiment, com-Friendster with $N$=64M and $K$=12K, each
requires~3TB. Analogously to the single-machine optimization described in
Section~\ref{sec-gpu}, we store only $\pi$ and
$\sum\phi$, and recalculate
$\phi$ from these whenever appropriate. Here, we trade a substantial gain
in memory usage against some computation. $\pi$ is partitioned across the workers,
and $\pi$ values are accessed via a DKV (distributed key-value) store.

% lack of locality

The distribution within the graph of the vertices of the mini-batch as well as
the neighbor sets is completely random. That means that there is \textit{no
locality}
in $\pi$ access patterns, especially in the accesses to the neighbor sets
$\Neighbors$.
Hence, there is no opportunity to exploit data locality through caching of
$\pi$.

\subsection{RDMA Distributed Key-Value Store}

%\looseness=-1
Modern network technology allows for Remote Direct Memory Access (RDMA)
through the network cards (NICs) with negligible
overhead~\cite{Hamada_infinibandtrade,Beck:2011:PER:2043535.2043537}. The use
of RDMA enables communicating peers to avoid the high latency for
interrupting a processor and waiting for it to act upon a request. Instead, a
process registers a region of memory that can be directly accessed by the
NIC. % RDMA only provides primitive operations and limited abstractions.
This has opened up the development of
efficient remote memory services, often in the form of distributed key-value
(DKV) stores.

The distributed algorithm implementation stores $\pi$ in a DKV store in the
collective memory of the
cluster, where $\pi[i]+\sum\phi[i]$ is the value for key $i$.
%
We decided not
to use any of the existing implementations of RDMA DKV stores, and instead
build our own DKV store on top of the Infiniband ib-verbs~API.
%
This is motivated by the fact that our use case is unique in a number
of important aspects. First, the number of keys and values to be stored are
constant throughout the execution of the algorithm. For instance, no records
are dynamically inserted or deleted from the DKV after their initial insertion,
although the value associated with each key can be updated. Additionally, all
keys are of the same type and represent a unique vertex in the graph.
Therefore, keys can be represented as integer identifiers. Moreover, each key
points to a vector of $K+1$ floats as its value.
%
This data representation can be exploited to simplify the management of
read and write requests. Similarly, the keys can be deterministically
partitioned across the cluster using a fast hash function.

The algorithm's access pattern is well controlled because the
computation is partitioned into stages that are separated by barriers. A
single stage either reads values or updates values. The algorithm ensures that updates
are always targeting unique elements,
therefore, there are no read/write or write/write hazards. The existing DKV
stores (RamCloud~\cite{Ousterhout:2015:RSS:2818727.2806887}, FaRM~\cite{179767},
and Herd~\cite{Kalia:2014:URE:2740070.2626299}) incur
overhead for insert/delete flexibility, concurrency control, or variable-sized
values. Our use case allows us to do any operation in exactly one RDMA read
or RDMA write.
%
Herd argues that replacing RDMA reads by
an RPC to the server, followed by an unreliable write, is faster.
%
However,
the paper shows this only holds for small payload packets, up to 256B. A $\pi$
packet in our application is typically thousands to hundreds of thousands
of 4-byte floats.

% each of the stages

\subsection{Implementation of distributed parallelism}

This section describes the parallelization of each of the stages of the
algorithm's main loop. All but the first stage justify parallelization for
high values of combinations of the parameters mini-batch size $|\Minibatch|$,
number of vertices in the mini-batch $M$,
number of communities $K$, neighbor sample size $|\Neighbors|$, and held-out set
size $|\cE_h|$.

The first stage, mini-batch selection (line 3 in Algorithm~\ref{algorithm}),
is done by the master and it is
not itself parallelized. However, the distributed implementation overlaps
its execution using pipeline parallelism while the workers are
performing \textit{update\_phi}. The mini-batch is partitioned equally over
the workers and the relevant sections of \Edges are distributed together with the
mini-batch subsets.

After a worker has received its subset of the mini-batch, it samples a neighbor
set $\Neighbors$ for each of its mini-batch vertices, using thread parallelism
(line 5 in Algorithm~\ref{algorithm}).

The next stage, \textit{update\_phi} (line 6 in Algorithm~\ref{algorithm}), is the algorithm's dominant stage, not
only in calculation but also in memory accesses. It performs $M \times
|\Neighbors| \times
K$ operations. It can be fully parallelized because it is a data-parallel
operation over each of the mini-batch vertices. \textit{update\_phi} loads the
$\pi$ values for the local mini-batch vertices and their neighbors from the
DKV store. The updates to $\phi$ for the mini-batch vertices are calculated
independently using thread parallelism.

%\looseness=-1
These updates are used by \textit{update\_pi} (line 9 in
Algorithm~\ref{algorithm}) locally, besides the value of
$\pi/\phi$ for the mini-batch vertices. The update to $\pi$ requires
$M \times K$ operations, and it is done in parallel over mini-batch vertices. Because of
memory consistency, this stage awaits completion of \textit{update\_phi} with
an MPI barrier. After the calculation, the updated values of $\pi,~\sum\phi$
are written to the DKV store.

\textit{update\_beta} (lines 12 and 15 in Algorithm~\ref{algorithm}) requires $|\Minibatch| \times K$ operations. It is preceded by an MPI
barrier to ensure that up-to-date $\pi$ values are read. \textit{update\_beta}
is split into four steps. The first stage partitions the mini-batch across
machines to calculate contributions to $g_{ab}(\theta)$. The values of $\pi$ for
the local mini-batch vertices are loaded from the DKV store. These calculations
are done with thread parallelism over the mini-batch vertices. In the second step,
a multi-threaded summation is performed to calculate the contributions per
machine, followed by a distributed summation using an MPI reduce operation. The
third step calculates $\beta$ from $\theta$ sequentially at the master. This
operation takes only $K$ steps. The resulting $\beta$ is broadcast with MPI
to the workers.

The calculation of perplexity requires $|\cE_h|\times K$ steps. Each of the
machines owns a subset of $\cE_h$. It loads up-to-date values of $\pi$ for each
vertex in its part of $\cE_h$, then for each of its edges in $\cE_h$ it calculates the
contribution to the perplexity using thread parallelism. The resulting
contributions are summed to the global perplexity value in two stages,
with a local OpenMP reduction followed by a global MPI reduction.

Note that the calculation requirements per iteration do not depend
on $N$ or $|\cE^*|$, the size of the network graph. However, larger graphs are
expected to require more iterations to achieve convergence.

\subsection{Pipelining of computation and loading $\pi$}
\label{sec-pipelining}

The distributed design features two instances of pipeline parallelism. First,
at the process level, sampling of the minibatch by the master is decoupled
from \textit{update\_phi} by the workers, so the master computes the minibatch
for the next iteration while the workers compute \textit{update\_phi} for the
current iteration. Second, at the threads level within the workers, loading of
$\pi$ and \textit{update\_phi} are split up into chunks, and the calculation
in \textit{update\_phi} for the current chunk is done simultaneously with
loading $\pi$ for the next chunk. The rationale for implementing pipeline
parallelism in these places of the program is that loading $\pi$ is the
dominant contribution within the dominant stage \textit{update\_phi}, as
will be shown in Section~\ref{eval-pipeline}.  Since loading $\pi$ is the
performance bottleneck of the distributed algorithm, performing other work
in parallel shortens the critical path.

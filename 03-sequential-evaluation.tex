\subsection{Performance of Sequential Versions}
\label{sec-seq-performance}

This subsection presents the results of transforming the algorithm
implementation from Python to efficient C++.
Our experiments use a DAS5 compute node, see Section~\ref{sec-experimental}.
%
We also investigate the performance impact of using
32-bit floating point numbers in stead of 64-bit doubles.
This hardly reduces the computation intensity for a DAS5 Xeon machine, but
it certainly affects data intensity because the data sizes are halved.
It has been previously shown that stochastic learning algorithms do not require
high precision in the presence of statistical approximations and the addition
of random noise~\cite{DBLP:journals/corr/GuptaAGN15}.
%
% It has been previously shown that stochastic learning
% algorithms do not require high precision in the presence
% of statistical approximations and the addition of random
% noise [?].

Due to the performance limitations of the Python implementation, the
experimental configuration of this section is very small.
%
We use the CA-HepPh dataset from the SNAP graph collection, see
Table~\ref{table-snap} in Section~\ref{sec-experimental}. The number of
iterations is~1000;
%Note that these trial runs stop very far before convergence; using a small
%number of
%iterations is valid because the time spent in an iteration does not vary
%during the lifetime of the algorithm.
the number of communities is $K=1024$, the mini-batch size is fixed at $M=32$, the
neighbor sample size $\Neighbors=32$.

\begin{comment}
******** Unnecessary detail?
The mini-batch sampling as described in Section~\ref{sec-background} randomly
chooses either a mini-batch of link edges, whose size is the degree of one
randomly selected vertex, or a mini-batch of nonlink edges, whose size is
specified as a model parameter. In this evaluation, we only select batches
of nonlink edges because that makes the mini-batch size, and hence the
execution times, deterministic. We separately validated that the time spent
\textit{per mini-batch vertex} for samples of link edges and nonlink edges
is fully consistent.
\end{comment}

\begin{table}[tb]
\center\begin{tabular}{l d{5.1} d{2.1} d{2.1} d{2.1}}
	&	& \multicolumn{3}{c}{C++} \\
\cline{3-5}
	& & 
            \multicolumn{1}{c}{Python} &
                \multicolumn{1}{c}{float64} &
		    \multicolumn{1}{c}{float32} \\
\multicolumn{1}{l}{Algorithm stage} &\multicolumn{1}{c}{Python} &
            \multicolumn{1}{c}{idiom} &
                \multicolumn{1}{c}{(baseline)} &
		    \multicolumn{1}{c}{baseline} \\
\hline
sample mini-batch       &      0.03 &  0.03 &  0.03 & 0.02 \\
sample neighbor sets    &      8.5  &  0.5  &  0.4  & 0.35 \\
update\_phi             &  9,014    & 52.7  &  8.9  & 4.9  \\
update\_pi              &     93.1  & 40.3  &  0.11 & 0.05 \\
update\_theta\_beta     &    339    &  2.1  &  0.76 & 0.55 \\
perplexity*             &    178    &  0.18 &  0.18 & 0.26 \\
\hline
\\[-1ex]
\end{tabular}
\caption{Performance comparison between Python; C++ that follows the Python
idioms; and our baseline C++ implementations. Times in
\textrm{ms} per iteration; *perplexity time divided by 100 iterations.}
\label{table-python-comparison}
\end{table}

Table~\ref{table-python-comparison} compares performance between Python,
the C++ version labeled 'Python Idiom' with the inefficiencies inherited from
Python, then our baseline C++ version which has the inefficiencies removed,
and finally the same with 32-bit floats.
%
We present timings for the compute stages of the algorithm described in
Algorithm~\ref{algorithm}, with the exception of
\textit{update\_theta\_beta} which combines \textit{update\_theta}
and \textit{update\_beta}.
%
% The numbers are times in ms. For all stages except perplexity, the times
% are per iteration.
%
% Once every so many iterations (hundreds or thousands in production runs),
% the perplexity calculation is invoked. The perplexity time in the table
% has been divided by 100 iterations; per perplexity invocation, it is large in
% comparison with the iteration stage times, but it is amortized over those
% iterations.
In production runs, the perplexity calculation is invoked once every hunderds
or thousands iterations. Hence, the perplexity timings are amortized over
100~iterations.

In all implementations, \textit{update\_phi} dominates the computation.
The translation from Python into 'Python Idiom' C++ speeds up this stage
by a factor~171. % , even though the Python implementation uses numpy for its data
% structures. 
Our C++ optimizations speed up this stage
by another factor of~6. Reducing the floating-point precision to 32~bits
doubles this factor, which again shows that the algorithm is very much data-dominant.
%
\textit{update\_pi} has a disproportionately large speed
gain after removing inefficiencies. The numpy implementation updated the full
$\pi$ array (incidentally, rather efficiently), whereas our C++ optimization
limits the updates to only those values of $\pi$ that have changed.
%
\textit{update\_theta\_beta} gave fewer opportunities for C++ optimization,
in the sampling stages there was none. In \textit{mini-batch-sampling} there
is no speedup compared to Python because it consists of a call to
a fast Python random primitive that cannot be bettered from C++.
The total performance gain between Python and the 64-bit float baseline C++
implementation exceeds a factor of~1000, and this grows to a factor of over~1500
if the floating-point precision is reduced to 32~bits. The 32-bit precision
implementation will be the baseline for the parallel performance comparison.

\subsection{Multi-threading}

A multi-threaded version was created by annotating loops in the
baseline implementation with OpenMP primitives in a straightforward
fashion. This accelerated the stages \textit{update\_phi} and
\textit{update\_pi} with a factor~4 for the very small problem instance
in Table~\ref{table-python-comparison}. For larger problem instances, the
speedup for these dominant phases grew to a factor~10. The vectorizing OpenCL
backend described in the next Section again significantly improves the
performance, but at the cost of substantial coding effort.
\textbf{\emph{Question: speak about the other stages? they are pipelined.}}

\begin{comment}
The introduction of a custom user-space random generator brings at most
a very small
benefit. We show it, because it is necessary for the multi-threaded
implementations described in the next section, and this measurement serves to
prove that it does not harm execution speed.
\end{comment}

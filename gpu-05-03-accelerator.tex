\begin{table}[tb]	% [htb]
% \center\begin{tabular}{l c c c c c c c c d{3.1}}
% Device & Architecture & cores & Clock & \multicolumn{2}{c}{GFlops} & L2 Cache & Memory & Bandwidth \\ 
%        &              &       & MHz   & single & double            & KB       & GB     & (GB/s) \\ 
% \hline
% K20m        & Tesla   &  2496 &  706        & 3520 & 1170 &      &  5 & 208 \\
% K40c        & Tesla   &  2880 &  745        & 4290 & 1430 &      & 12 & 208 \\
% GTX 980     & Maxwell &  2816 & 1126 (1216) & 4612 &  144 &      &  4 & 336.5 \\

% GTX Titan-X & Maxwell &  3072 & 1000 (1075) & 6144 &  192 & 2048 & 12 & 336.5 \\
%
%
%###########################################################################################
% SPECS FROM:
% http://www.geforce.com/hardware/desktop-gpus/geforce-gtx-titan-x/specifications
% http://www.geforce.com/hardware/desktop-gpus/geforce-gtx-980/specifications
% http://www.nvidia.com/content/PDF/kepler/Tesla-K40-Active-Board-Spec-BD-06949-001_v03.pdf
% http://www.nvidia.com/content/PDF/kepler/Tesla-K20-Active-BD-06499-001-v04.pdf
%
% FLOPS DATA:
% K40: http://international.download.nvidia.com/pdf/kepler/TeslaK80-datasheet.pdf
% K20 & K40: http://www.nvidia.com/content/tesla/pdf/nvidia-tesla-kepler-family-datasheet.pdf
% GTX980 & TITANX: https://en.wikipedia.org/wiki/List_of_Nvidia_graphics_processing_units
\setlength\tabcolsep{0.4em}
\center\begin{tabular}{lc c c c c c}
                         && \multicolumn{5}{c}{Device} \\
\cline{3-7}
                && \multicolumn{2}{c}{GTX Titan-X} & GTX 980 & K40c  & K20m  \\
Specification            && (Pascal) & (Maxwell)   &         &       &       \\
\cline{1-1}\cline{3-7}
Number of Cores          && 3584     & 3072        & 2048    & 2880  & 2496  \\
Clock (MHz)              && 1417     & 1000        & 1126    & 745   & 706   \\
GFlops (single)          && 10157    & 6144        & 4612    & 4290  & 3520  \\
GFlops (double)          && 317      & 192         & 144     & 1430  & 1170  \\
Memory (GB)              && 12       & 12          & 4       & 12    & 5     \\
Bandwidth (GB/s)         && 480      & 336.5       & 224     & 288   & 208   \\
\cline{1-1}\cline{3-7}
\\[-1.5ex]
\end{tabular}
\caption{Properties of the GPUs used in the evaluation}
\label{table-gpus}
\end{table}

\subsubsection{Analysis of GPU Parallelism}
As discussed in Section~\ref{gpu-design}, there are 10 distinct flavors of the
GPU version of the algorithm: the naive strategy, the
shared strategy, and 8 variations of the code generation strategy. This
section investigates the effectiveness of each flavor on NVidia's GTX~\mbox{Titan-X}
(Maxwell and Pascal),
GTX980, K40c and~K20m.

% TITANX
\begin{figure*}[tb]	% [htb]
  \centering
  \epsfig{file=plots/gpu-sweep-legends.eps, width=1.0\textwidth}
  \setlength\tabcolsep{0em}
  \begin{tabular}{c c c c}
  \epsfig{file=plots/titanx-maxwell-1024-strategies-w1.eps, width=0.25\textwidth} &
  \epsfig{file=plots/titanx-maxwell-2048-strategies-w1.eps, width=0.25\textwidth} &
  \epsfig{file=plots/titanx-pascal-1024-strategies-w2.eps, width=0.25\textwidth} &
  \epsfig{file=plots/k40-1024-strategies-w2.eps, width=0.25\textwidth}
  \end{tabular}
  \caption{Execution time of 1000 \textit{update\_phi} invocations across a sweep of
  	\textit{update\_phi} thread block sizes. The lower figures are a zoom-in into
	the optimal block range of the upper figures.
	Other relevant model parameters: M=4096, $|\Neighbors|$=32.
	% (a) \mbox{Titan-X(Pascal)} GPU, with kernel vectorization of width 2;
	% Relevant model parameters: K=1024, M=4096, $|\Neighbors|$=32.
	% (b) \mbox{Titan-X} GPU, without explicit kernel vectorization;
	% Relevant model parameters: K=1024, M=4096, $|\Neighbors|$=32.
  	% (c) \mbox{Titan-X} GPU, without explicit kernel vectorization;
	% K=2048, M=4096, $|\Neighbors|$=32.
	% (d) K40c GPU, with explicit kernel vectorization of width 2;
	% K=1024, M=4096, $|\Neighbors|$=32.
  }
  % \label{titanx-w1-sweep}
  \label{gpu-sweep}
\end{figure*}

\begin{comment}
% TITANX K=2K
\begin{figure*}[tb]	% [htb]
  \centering
  \epsfig{file=plots/titanx-maxwell-2048-strategies-w1.eps, width=\textwidth}
  \caption{Execution time of 1000 \textit{update\_phi} invocations using the
  \mbox{Titan-X(Maxwell)} GPU,
  without explicit kernel vectorization, across a sweep of
  \textit{update\_phi} thread block
  sizes. Relevant model parameters: K=2048, M=4096, $|\Neighbors|$=32.}
  \label{titanx-w1-sweep-2k}
\end{figure*}
\end{comment}

Figure \ref{gpu-sweep}(a) presents the performance of the Maxwell \mbox{Titan-X} GPU
for all 10 strategy flavors, with an explicit vector width~2 in the kernels. The
x-axis represents different \textit{update\_phi} thread block sizes while the y-axis presents
the total execution time of 1000 invocations of the \textit{update\_phi} kernel. The
naive and shared strategies are labeled NAIVE and SHARED respectively. Further,
each flavor of the code generation strategy is labeled by GEN followed by the 3
choices that identify it.
%
The wide performance range inhibits the readability of the figure,
therefore, a zoomed-in version of Figure~\ref{gpu-sweep}(a) is provided as the bottom figure. As would
be expected, the naive strategy exhibits the worst performance over all
thread block
sizes, as it does not explicitly cache repeated device memory read operations.
The SHARED and \textit{GEN-SSS} strategies come next in terms of performance.
Both strategies cache $Grads_i$, $Probs_i$ and $\pi_i$ in shared memory but
differ in one aspect: \textit{GEN-SSS} explicitly unrolls the internal
loops of the kernel. However, there is no significant performance difference
between them. The other flavors of the code generation strategy attain higher
performance as they unroll internal loops as well as cache data in
registers.
The Maxwell Titan-X in Figure~\ref{gpu-sweep}(b) obtains the best performance with the \textit{GEN-RSS}
strategy and a thread block size of 64 and no vectorization. The results of other
vector widths
are omitted as they obtain lower performance.

A key model parameter that affects the behavior of the optimization strategies
is the number of communities~$K$. Figure~\ref{gpu-sweep}(c) presents the
same model configuration as in Figure~\ref{gpu-sweep}(b) but $K=2048$
instead of $K=1024$. The most important difference in performance between the two figures is the optimal
thread block size. An increase in $K$ comes with a proportional increase in
the size of shared memory space that is used by each thread block for the
strategies that employ shared memory. Similarly, GEN strategies that use the
register file will require additional space. Therefore, the number of
concurrent thread blocks that can execute on a single streaming multiprocessor
will decrease, minimizing the GPU's occupancy and utilization. This effect is
most clear when comparing the NAIVE with the SHARED and GEN-SSS strategies. In
this case, the NAIVE strategy outperforms both SHARED and GEN-SSS for a
thread block
size of 32 due to their low occupancy. This limitation can be counteracted by
selecting a larger thread block size which in turn increases the computation
concurrency and occupancy.

The Pascal \mbox{Titan-X} in Figure~\ref{gpu-sweep}(c) is the fastest of the GPUs
under investigation, note the y-axis range. The NAIVE strategy yields relatively
worse performance in comparison to the other GPUs. In contrast to the Maxwell
\mbox{Titan-X}, the Pascal shows very good performance with the SHARED strategy.
This GPU performs best with the \textit{GEN-RSR} strategy, a block size of~96
and a vector width of~2.

\begin{comment}
% K40
\begin{figure*}[tb]	% [hbt]
  \centering
  \epsfig{file=plots/k40-1024-strategies-w2.eps, width=\textwidth}
  \caption{Execution time of 1000 \textit{update\_phi} invocations using the K40c GPU, with
  explicit kernel vectorization of width 2, across a sweep of
  \textit{update\_phi} thread block
  sizes. Relevant model parameters: K=1024, M=4096, $|\Neighbors|$=32.}
  \label{k40-w2-sweep}
\end{figure*}
\end{comment}

% \looseness=-1
Figure~\ref{gpu-sweep}(d) presents the performance of the K40c GPU
for the same experimental configuration as shown for the Pascal \mbox{Titan-X} in
Figure~\ref{gpu-sweep}(c), again with a vector width of~2. The results
for the other code versions with vector width 4 and no vectorization are
omitted as they exhibit lower performance.
%
Surprisingly, Figure~\ref{gpu-sweep}(c) shows that the NAIVE strategy
outperforms SHARED and some of the GEN strategies. This can be explained by the
unique properties of the Tesla Super Computing line of products to which the
K40c belongs. These GPUs include enhanced L2 caching mechanisms that
accelerate repeated and sparse memory accesses. This is especially advantageous
as it caches repeated reads across streaming multiprocessors. However, the
highest performance is attained by \textit{GEN-RRS} which explicitly employs
registers for both $Probs_i$ and $Grads_i$.

These performance results for a range of GPUs
reinforce the importance of customizing compute kernels to each GPU's specific
architecture and capabilities. For instance, each GPU achieved its highest
performance by employing a different strategy. Moreover, each GPU displayed
different strategy-performance orderings.

\begin{figure}[tb]	% [htb]
  \centering
  \epsfig{file=plots/gpus.eps, width=0.9\columnwidth}
  \caption{Speedup comparison of best performing parallel configurations of
  each compute device normalized over baseline C++ version. Relevant model
  parameters: K=1024, M=4096, $|\Neighbors|$=32.}
  \label{gpus-fig}
\end{figure}

\begin{figure}[tb]	% [htb]
  \centering
  \epsfig{file=plots/gpu-vector-sweep.eps, width=0.9\columnwidth}
  \caption{Performance of the best performing strategy per GPU, varying the
  vector width, normalized over the non-vectorized kernel. Relevant model
  parameters: K=1024, M=4096, $|\Neighbors|$=32.}
  \label{gpus-v-sweep}
\end{figure}

\subsubsection{Comparison of Compute Devices}
Figure~\ref{gpus-fig} compares the highest speedup achieved by each of the
GTX~\mbox{Titan-X} Maxwell and Pascal, GTX980, K40c and K20m relative to the
single-threaded baseline C++ version. These results are consistent with the relative
capabilities of each device as listed in Table~\ref{table-gpus}. For instance,
the Pascal \mbox{Titan-X} achieves the highest speedup of 120 relative to the baseline.

As vectorization is a popular GPU optimization, it is important to note that
the Maxwell \mbox{Titan-X} and GTX980 achieve their highest performance with non-vectorized kernels.
On the other hand, the Pascal \mbox{Titan-X} and Tesla GPUs obtain the best performance with a vector
width of~2. Figure~\ref{gpus-v-sweep} presents the execution time of the best
performing strategy for each of the 4 GPUs. In this figure, the performance is
normalized over the non-vectorized kernel version for each GPU. At one extreme,
the Maxwell \mbox{Titan-X} exhibits an overhead factor of nearly 1.8 when using a vector
width of~4. On the other hand, the K40c improves its performance by roughly
20\% when it uses a vector width of~2 compared to the non-vectorized kernel.
Therefore, explicit vectorization of the kernels can be either useful or
harmful depending on the GPU architecture and the specific problem it is
applied to.




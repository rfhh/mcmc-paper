\section{Related Work}
\label{sec-related}

We consider related work for both parts of this paper: acceleration and scaling
of our MCMC-MMSB algorithm. Acceleration of deep learning algorithms on GPUs
is an ongoing success story. Although approximative Baysian algorithms are
not such a natural fit for GPUs, a number of projects explores this terrain.
Medlar~c.s.~\cite{journals/bioinformatics/MedlarGSBK13} use GPUs with their
MCMC approach to analyze parental linkage patterns in a biology context and
White and Porter~\cite{DBLP:journals/csda/WhiteP14} do the same to model terrorist activity.
Latent Dirichlet Allocation, another variety of Bayesian Approximation,
is used on GPUs by Yan~c.s.~\cite{DBLP:conf/nips/YanXQ09}. There is also related
work on the variants of MCMC algorithms that use the gradient to speed
up convergence. Langevin and Hamiltonian dynamics are representatives of
these varieties~\cite{Girolami_riemannmanifold}. Our algorithm uses Riemann
Manifold Langevin dynamics. Beam~c.s.~\cite{beam2014fast} use GPUs to perform
Hamiltonian descent, using Python interfaces to access the standard cuBLAS
library~\cite{cuBLAS}. They limit their optimizations for the GPU to saving
on data transfers between host and device memory. They do not apply their
framework to MMSB.

A different MMSB algorithm with stochastic gradient descent on the GPU is
the Online Tensor approach~\cite{DBLP:journals/corr/HuangNHVA13}. They deduce
overlapping communities for many of the same data sets as this paper, and they
report fast convergence. Their implementation uses the cuBLAS
library, and, unlike our work, there is no attempt to hand-optimize the GPU
kernels. Additionally, the algorithmic approach to solve the problem is
fundamentally different.
Since they target GPUs only, the data sets they
can handle are limited by the device memory of the GPU. Our implementation converges
quickly on the GPU but can also use multi-core CPUs. In the latter case, speedup is
limited but the larger memory allows tackling larger data sets.

Liakos~c.s.~\cite{liakos-bigdata16} also address overlapping community detection, but their
algorithm uses a local dispersion-aware approach. They find communities for 5000
nodes in each of the large SNAP data sets, see Table~\ref{table-snap}.

\begin{comment}
Since our SG-MCMC algorithm is completely different from deep-learning
algorithms, it makes no real sense to compare to the learning frameworks
Theano, Caffe, cuDNN, or NVidia Digits, even though they target GPUs.

SG-MCMC work NOT on GPUs:

GPU work:
MCMC Hamiltonian:
	1. Andrew L. Beam, Sujit K. Ghosh, Jon Doyle
		Fast Hamiltonian Monte Carlo Using GPU Computing
		http://arxiv.org/pdf/1402.4089.pdf
MMSB tensor:

MCMC (not SG):
    1. Alan Medlar, Dorota GÅ‚owacka, Horia Stanescu, Kevin Bryson, Robert Kleta
       SwiftLink: Parallel MCMC linkage analysis utilising multicore CPU and GPU
       http://bioinformatics.oxfordjournals.org/content/early/2012/12/13/bioinformatics.bts704.full.pdf
    2. Marc Suchard, Chris Holmes, Mike West
       Some of the What?, Why?, How?, Who?  and Where?  of Graphics Processing Unit Computing for Bayesian Analysis
       https://stat.duke.edu/gpustatsci/GPU-ISBABull2010.pdf
\end{comment}

\begin{comment}
In this paper, we describe our custom RDMA D-KV (Distributed Key-Value)
store. Current RMDA D-KV store implementations are RamCloud~\cite{RamCloud},
Pilaf~\cite{Pilaf}, Herd~\cite{Herd} and FaRM~\cite{FaRM}. All these systems
use RDMA to implement a D-KV store. However, all of them are far more powerful
than our custom implementation -- and this power comes at a cost that we
can avoid. They implement a generic D-KV store that controls concurrency,
supports dynamic inserts and deletes, supports variable-sized values
(whose size may change at an update), and keys of arbitrary type. Because
of the nature of our distributed algorithm, we have to deal with none of
these issues. For us, values are fixed-size, allocated only at the initial
population, and remain alive forever. We have no concurrency between writes
and reads or other writes. Our keys are a contiguous range of integers. All
these properties together allow an extremely low-overhead implementation
that does not involve the remote host in any transaction.
\end{comment}

There are a few projects that implemented a distributed algorithm
for community detection on very large network graphs. In contrast to our work,
they all detect non-overlapping communities: Bu~c.s.~\cite{Bu2013246} investigate a
large number of networks; Lyzinski~c.s.~\cite{2015arXiv150302115L} investigate hierarchical
stochastic blockmodels; Prat-P{\'e}rez~c.s.~\cite{Prat-Perez:2014:HQS:2566486.2568010} use
multi-core machines.

\textbf{\emph{QUESTION: Re-insert related work on RDMA KV stores?}}
\begin{comment}
\subsection{Related work on RDMA Key-Value stores}
In this paper, we describe our custom RDMA D-KV (Distributed Key-Value)
store. Current RMDA D-KV store implementations are RamCloud~\cite{RamCloud},
Pilaf~\cite{Pilaf}, Herd~\cite{Herd} and FaRM~\cite{FaRM}. All these systems
use RDMA to implement a D-KV store. However, all of them are far more powerful
than our custom implementation -- and this power comes at a cost that we
can avoid. They implement a generic D-KV store that controls concurrency,
supports dynamic inserts and deletes, supports variable-sized values
(whose size may change at an update), and keys of arbitrary type. Because
of the nature of our distributed algorithm, we have to deal with none of
these issues. For us, values are fixed-size, allocated only at the initial
population, and remain alive forever. We have no concurrency between writes
and reads or other writes. Our keys are a contiguous range of integers. All
these properties together allow an extremely low-overhead implementation
that does not involve the remote host in any transaction.
\end{comment}

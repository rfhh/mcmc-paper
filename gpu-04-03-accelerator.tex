% \section{Accelerating on one Many-core Machine}

\subsection{Design of Accelerator Optimizations}

% Accelerator friendliness
The design of an accelerated version of the MCMC algorithm necessitated several
crucial modifications to allow for efficient parallelization. This section
discusses the key design contributions to attain two optimized
parallel versions, catering for multi-core CPUs and GPUs. First, we present
common structural changes that provide a foundation for compute device specific
optimizations. Next, we provide an in-depth description of the optimized CPU
and GPU versions respectively.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Using OpenCL and CUDA}
%OpenCL/CUDA
Initially, the system was developed using OpenCL~\cite{opencl} in order to
accelerate the computations of the algorithm. OpenCL was chosen as it provides
a common abstraction for a variety of compute devices which fulfilled our
requirements of employing CPUs or GPUs. However, NVidia's OpenCL SDK limits the
total memory allocations within a context to 4GB which severely limits the
problem sizes that can be tackled on GPUs. Therefore, we migrated the system to
use the abstraction layer CLCudaAPI~\cite{claduc}
to support both OpenCL and CUDA~\cite{Nickolls:2008:SPP:1365490.1365500}
as back-ends.
%
CLCudaAPI
hides the difference between OpenCL and the CUDA driver API.
For example, the library provides abstractions for Device, Context, Queue and
Buffer. Additionally, the implementations use a combination of inline functions
and preprocessor macros to abstract away the differences between OpenCL's and
CUDA's primitive types, address spaces and functions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Fast Lookup of Graph Edges}
% Set/Cuckoo Hash

The algorithm relies on a set data structure to store the edges of the graph.
This set is queried frequently with randomly generated edges to check for their
membership. In order to improve the performance of such lookups, a custom set
implementation was developed that restricts the set's features based on an
observation of its usage patterns.
The graph is immutable, hence its maximum
size can be specified at creation time and need not be adjustable or provide
thread-safety guarantees. The set data
structure is built as a variant of a cuckoo
hash~\cite{Pagh:2004:CH:1006424.1006426} where each element is a 64-bit
field containing a tuple of the two 32-bit vertices. The cuckoo hash
uses two hash functions to index their corresponding spaces. Additionally, the
cuckoo hash allows for 4 keys per bucket. This hash design allows for a load
factor upwards of 90\% which allows a very economical memory usage.
%% FIXME: Talk lookup hit/miss with 4 keys per bucket?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Parallelization \& Data Dependencies}
% Division into kernels
The original algorithm was structurally reorganized into 4 main sections with
one or more kernels in each depending on data dependencies and the applicable
synchronization requirements.
%
First, the sampling of a mini-batch of edges is done on the host as it is a
cheap operation. The mini-batch sampling is followed by the neighbor sampling
kernel which generates uniformly random neighbors for each vertex in the
mini-batch. 
%
Second, the predominant kernel, \textit{update\_phi} is invoked to calculate the
gradients for each vertex $i$ in the mini-batch \Minibatch and updates the value of $ \phi_i
\mid i \in \Minibatch$. Next, the \textit{update\_pi} kernel is invoked to normalize the
individual $\phi_i$ and store the result in the corresponding~$\pi_i$.
%
Third, the kernels \textit{update\_theta} and \textit{update\_beta} are invoked to modify the
global parameters.
%
Finally, the perplexity calculation is performed in a dedicated kernel.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Memory Footprint Reduction}
% Memory footprint halved

A key structural change applied to the algorithm is the lossless compression of
its state. This modification enabled the algorithm to process larger data sets
while maintaining all program state in memory. Moreover, as the algorithm is
data-intensive, a reduction of the state is accompanied by a decrease in its
data-intensity.
%
The data structures that occupy most memory are $\pi$ and~$\phi$, both of
which are matrices of dimensions ${N}\times{K}$. However, the storage of both
matrices is redundant as $\pi$ is a row-normalized copy of~$\phi$, see
Table~\ref{tbl-symbols} in Section~\ref{sec-background}. The full storage for
$\phi$ is discarded; $\phi_{ik}$ values are calculated as
$\pi_{ik}/\phi^{sum}_i$, which requires maintenance of a vector
$\phi^{sum}$ of size $N$.
Moreover, the calculation for $\phi_{ik}$ can be cached usefully.
%
The $\phi$ matrix is required in two kernels only, namely, \textit{update\_phi} and
\textit{update\_pi}. Both kernels access $\phi_i$ only for vertices~$i$ in the
mini-batch~\Minibatch, so for each iteration, the calculated values for $\phi$ are cached
in a smaller temporary matrix of size
$M\times{K}$.
%
% In this context, the initialization of the algorithm's state is performed by
% normalizing the full $\pi$ matrix in-place without the need for an additional
% matrix. Moreover, the normalization denominator of each row is cached in
% $\phi^{sum}_{i}$.
%
% Thus each read-only access to $\phi_{i,j}$ can be substituted with
% $\pi_{i,j}\phi_{i}^{sum}$. Given that the value of $\phi_{i,j}$ is consistently
% used in conjunction with $\pi_{i,j}$ and that $\phi_{i}^{sum}$ is read once and
% reused multiple times, this
This transformation trades memory storage and
bandwidth for a minimal computation overhead.
%
% On the other hand, the newly computed $\phi$ values can be stored in the
% temporary matrix and later be normalized into their corresponding rows of
% $\pi$.

Thus, the original memory requirement for $\phi$, ${N}\times{K}$, is
reduced to a vector $\phi_{sum}$ of length~$N$ and a comparatively small
$M\times{K}$ matrix. For sufficiently large~$K$, this transformation roughly
halves the memory footprint of the algorithm.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Single floating point vs double
Another modification to the original algorithm is migrating all of its
computation from double to single precision floating-point, analogous to the change
for the baseline implementation in Section~\ref{sec-seq-performance}.
%
This change affects the algorithm in multiple ways. For instance, on a GPU it reduces
both the computation and data intensity. It again reduces the memory
footprint of the algorithm and frees registers.

%Finally, the algorithm requires randomly generated values from 3 different
%distributions. 

\subsubsection{CPU-Specific Optimization}
The multi-core
CPU version of the algorithm uses OpenCL to perform its computations. The
work decomposition of the CPU kernels ensures that each thread performs
independent computation in order to avoid expensive synchronization operations.
Edge-centric kernels, which operate over mini-batch edges perform computations
over every edge in parallel.  Similarly, vertex-centric kernels that operate
over vertices in the mini-batch exploit parallelism across the selected
vertices.
% vectorization
Additionally, the kernels were vectorized to decrease instruction overhead and
utilize the SSE capabilities of the CPU cores.

\subsubsection{GPU-Specific Optimization}
\label{gpu-design}
The GPU implementation builds on top of the CPU work decomposition scheme.
However, instead of having every thread perform independent computations, each
block of threads shares the work associated with the single edge or vertex for
edge-centric and vertex-centric kernels.

Since all of the kernels are data-bound, several memory organization strategies
of varying complexities were investigated to speedup the algorithm by
exploiting the GPU's memory hierarchy.
% Update_phi
As a case study, a discussion of the \textit{update\_phi} kernel is provided as it is
the predominant component of the algorithm.

The \textit{update\_phi} kernel operates over every vertex~$i$ in the mini-batch and
requires two temporary vectors of length~$K$ to perform its computation. For
each vertex, it iterates over the randomly generated neighbors and computes a
vector of probabilities $Probs_i$ of length~$K$. The individual $Probs_i$ vectors
of each ($i$, neighbor) tuple are then used to update the gradients vector
$Grads_i$ for each vertex~$i$. Finally, the $\phi_i$ row is updated to
reflect the changes that were accumulated in $Grads_i$ for each vertex $i$ in the
mini-batch~\Minibatch.

A deeper analysis of the memory access patterns of the \textit{update\_phi} kernel
revealed the frequency and modality of access to the data structures.
%
The read-only accesses of $\pi_j$ of the randomly generated neighbors are unique
with a high probability. More precisely, each $vertex_i$ in the mini-batch
randomly samples a set of neighbor identifiers from the uniform distribution.
Given that the total number of sampled neighbors is much smaller than $N$,
there is a low likelihood of having duplicate samples which eliminates the
potential of data reuse. Therefore, these accesses provide limited or no
opportunities for optimization without interfering with the algorithm's
entropy.
%
The data structure usage patterns that are deterministic and most frequently
accessed in read/write mode are $Probs$ and $Grads$ respectively. Similarly,
$\pi_i$ for each vertex in the mini-batch is read repeatedly for the
calculation of $Probs$ per neighbor and again for the updates of $Grads$.

The following strategies present alternative methods of handling the
deterministic memory usage patterns of $Probs_i$, $Grads_i$ and $\pi_i$ for each
vertex in the mini-batch.

% Naive strategy
\paragraph*{\textbf{Naive strategy}} simply allocate temporary vectors in thread
local memory to store $Probs$ and $Grads$ which physically reside in
device memory. All memory accesses are coalesced to achieve the highest
possible bandwidth to and from device memory.

% Shared memory caching strategy
\paragraph*{\textbf{Shared memory strategy}} allocate the temporary vectors
$Probs_i$ and $Grads_i$
in shared memory. Furthermore, this strategy copies the $\pi_i$ of the selected
mini-batch vertex to shared memory in order to avoid repeated reads of the same
memory locations in device memory

% Register and shared memory using code generation and loop unrolling 
\paragraph*{\textbf{Code generation strategy}} dynamically generate the code of the kernel to
custom tailor its properties. For example, this strategy can control whether a
vector should be placed or cached in shared memory. Additionally, it can
control which vectors explicitly reside in registers by allocating space on the
stack frame, unrolling all inner loops of the kernel and substituting all
vector addressing with static values. The code generation strategy allows this
flexibility for the three vectors of concern, namely, $Probs_i$, $Grads_i$
and~$\pi_i$.
Hence, this strategy allows for 8 possible configurations denoted by
three letters each of which is a choice between $R$ or $S$ to represent
$Register$ or $Shared$ respectively. For example, $SSR$ denotes that $Probs_i$,
$Grads_i$ and $\pi_i$ were placed in $Shared$, $Shared$ and $Register$
respectively.

% vectorization
Additionally, we investigated the use of vector data types to decrease the
instruction overhead and increase memory bandwidth for all strategies.

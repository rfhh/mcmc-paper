\section{System Design}
\label{design-section}

\begin{comment}
- distributed parallelisation:
  cut up into stages like the CC-Grid paper:
  pipeline:
    1) sample mini-batch
    2) sample neighbors
    3) update_phi
  4) update_pi
  5) update_theta
  5a) calc_grads
  5b) sum_grads
  5c) update_theta
  6) update_beta
  7) perplexity

- DKV RDMA store

- data dependencies:
  mb = nodes in the mini-batch
  nb = nodes in the neighbor sets
  H  = held-out set

  	pi	phi	beta	theta	grad.th	G	prev.mini-batch
  1) 	-	-	-	-		R	-
  2) 	-	-	-	-		R	R
  3) 	R(mb)	R	R	-		R	-
	R(nb)
  4)	R/W(mb)	R	-	-		-	-
  5a)	R(mb)	-	R	R	W	-	-
  5b)	-	-	-	-	R/W	-	-
  5c)	-	-	-	R/W	R	-	-
  6)	-	-	W	R	-	-	-
  7)	R(H)	-	R(H)	-	-	H	-

- parallelization:
  1) sequential at master's, multithreaded
  2) parallel over mb; at workers', multithreaded
  3) parallel over mb; at workers', multithreaded
  4) parallel over mb; at workers', multithreaded
  5a) parallel at workers', multithreaded
  5b) omp_reduce(+) at workers', MPI reduce global
  5c) at master's, multithreaded
  6) broadcast theta by master, parallel update at workers'
  7) parallel over H; at workers', multithreaded
     reduce(+) at workers, MPI reduce global

- distributed parallelization:
  trade memory size for communication costs
  pity: the algorithm is already memory-dominant

- synchronization:
  in principle, each step synchronized
  but: pipelining in sample mini-batch / load pi/phi / update_phi

- data sizes:
  G has N vertices and |E| undirected edges
  |H| is 1..10% of |E|
  pi, phi, beta, theta are float32
  pi[N][K + 1] if phi folded into pi
  beta[K]
  theta[K][2]
  so, pi is the memory eater
   . beta/theta are replicated
   . phi is folded into pi
   . pi is partitioned across the workers
  G is stored at the master only: save the memory for pi so we can handle
  larger K. The relevant slices of G (m's adjacency lists) are scattered
  together with the mini-batch.
  G is stored as a Google SparseHash because c++ std::(unordered_)set has a
  huge overhead per item. Workers unpack their slice of G into a
  std::unordered_set because that is somewhat faster.

- pi storage: DKV
   . pi/phi is read in 3), 4), 5a), 7)
   . pi is written in 4); read[i], then write[i], so no other data dependencies
   . so access is very synchronous: either read-only, or write without
     concurrent readers
   . DKV store properties:
      - single-sized keys
      - contiguous key space (integers 0..N-1)
      - update-only writes: no new KVs, no deletes
     so: no load imbalance, no hashing that is worth its name
      - no read/write or write/write concurrency
     so:
      - build RDMA store with remote reads and remote writes only, with exactly
        one RDMA transaction per read or write
\end{comment}

% rationale -- here or in the intro?

% http://stanford.edu/~rezab/nips2014workshop/slides/jure.pdf

The SG-MCMC algorithm described in the previous section has an abundance of
opportunities for parallelism, for the multithreaded, shared-memory type as
well as for distributed-memory parallelism. The benefit of multithreaded
parallelism is speed-up of the computation. A distributed implementation
additionally allows to store data in the collective memory of the cluster
machine and increases memory bandwidth which scales with the number of
machines. The downside of a distributed implementation is that it requires
considerably more programming effort.

% intro parallelisation, distribution

This section describes how we parallelize the algorithm.
In most places, the usage of multithreaded parallelism
is straightforward, therefore, we will discuss details only where appropriate. The
distributed design follows a master-worker paradigm, where in essence
the master controls the parallel operations and the workers perform the
calculations. For thread parallelism, we annotate the program with
OpenMP~\cite{OpenMPSpec}. For distributed communication, we use
MPI~\cite{Forum:1994:MMI:898758}.

% largest data structures

\subsection{Data distribution}

As is common in machine learning problems, the edges of the graph are divided
into two sets, a training set $G$ and a held-out set $\cE_h$.
The largest data structures of the algorithm are $G$, $\cE_h$, $\pi$ and~$\phi$.
For the largest dataset in this paper, com-Friendster, $G$ has 1.8~billion
undirected edges. In our representation with directed edges, this takes
up 13.5GB. Our design lets $G$ reside only at the master. We observe that
the calculations in the update stages only require the subset of $G$ that
is touched by the mini-batch vertices, so the master scatters that subset to
the workers together with the scattering of the mini-batch vertices. This way,
we trade a reduction in memory at the workers against the cost of limited
communication. In contrast, $\cE_h$ can be partitioned statically over all
machines for the parallel perplexity calculation.

$\pi$~and $\phi$ are 32-bit float arrays of size $K \times N$. For our
largest distributed experiment, com-Friendster with $N$=64M and $K$=12K, each
requires~3TB. We decided to store only $\pi$ and $\pi^{sum}$, and recalculate
$\phi$ from these whenever appropriate. Here, we trade a substantial reduction
in memory against limited computation. $\pi$ is partitioned across the workers,
and $\pi$ values are accessed via a DKV (distributed key-value) store.

% lack of locality

The distribution within the graph of the nodes of the mini-batch as well as
the neighbor sets is completely random. That means that there is no locality
in $\pi$ access patterns, especially in the accesses to the neighbor sets.
Hence, there is no opportunity to exploit caching of $\pi$ through data
locality.

% each of the stages

\subsection{Implementation of distributed parallelism}

This section describes the parallelization of each of the stages of the
algorithm's main loop. All but the first stage justify parallelization for
high values of combinations of the  parameters $|\cV_n|$, $K$, $n$, and $|\cE_h|$.

The first stage, mini-batch selection (line 3 in Algorithm~1), is done at the master and it is
not itself parallelized. However, the distributed implementation overlaps
its execution using pipeline parallelism while the workers are
performing \textit{update\_phi}. The mini-batch is partitioned equally over
the workers and the relevant sections of $G$ are distributed together with the
mini-batch subsets.

After a worker has received its subset of the mini-batch, it samples a neighbor
set of size $n$ for each of its mini-batch vertices, using thread parallelism
(line 5 in Algorithm~1).

The next stage, \textit{update\_phi} (line 6 in Algorithm~1), is the algorithm's dominant stage, not
only in calculation but also in memory accesses. It performs $M\times n\times
K$ operations. It can be fully parallelized because it is a data-parallel
operation over each of the mini-batch vertices. \textit{update\_phi} loads the
$\pi$ values for the local mini-batch vertices and their neighbors from the
DKV store. The updates to $\phi$ for the mini-batch vertices are calculated
independently using thread parallelism.

These updates are used by \textit{update\_pi} (line 7 in Algorithm~1) locally, besides the value of
$\pi/\phi$ for the mini-batch vertices. The update to $\pi$ requires $M\times
K$ operations, and it is done in parallel over mini-batch vertices. Because of
memory consistency, this stage awaits completion of \textit{update\_phi} with
an MPI barrier. After the calculation, the updated values of $\pi+\pi^{sum}$
are written through the DKV store.

\textit{update\_beta} (lines 10 and 11 in Algorithm~1) requires $M\times K$ operations. It is preceded by an MPI
barrier to ensure that up-to-date $\pi$ values are read. \textit{update\_beta}
is split into four steps. The first stage partitions the mini-batch across
machines to calculate contributions to $\nabla\theta$. The values of $\pi$ for
the local mini-batch vertices are loaded from the DKV store. These calculations
are done with thread parallelism over the mini-batch vertices. In the second step,
a multi-threaded summation is performed to calculate the contributions per
machine, followed by a distributed summation using an MPI reduce operation. The
third step calculates $\beta$ from $\theta$ sequentially at the master. This
operation takes only $K$ steps. The resulting $\beta$ is broadcast with MPI
to the workers.

The calculation of perplexity requires $|\cE_h|\times K$ steps. Each of the
machines owns a subset of $\cE_h$. It loads up-to-date values of $\pi$ for each
vertex in its part of $\cE_h$, then for each of its edges in $\cE_h$ it calculates the
contribution to the perplexity using thread parallelism. The resulting
contributions are summed to the global perplexity value in two stages,
with a local OpenMP reduction followed by a global MPI reduction.

Note that the calculation requirements per iteration do not depend
on $N$ or $|E|$, the size of the network graph. However, larger graphs are
expected to require more iterations to achieve convergence.

\subsection{Pipelining of computation and loading $\pi$}

As will be shown in Section~\ref{eval-pipeline}, loading $\pi$ is the
dominant contribution within the dominant stage \textit{update\_phi}. This
is the performance bottleneck of the distributed algorithm. We introduce
pipeline parallelism in order to shorten the critical path. Loading $\pi$
and calculating \textit{update\_pi} are partitioned into chunks, so the
calculation of a chunk is overlapped with loading the next chunk of $\pi$
values, and also with sampling the mini-batch at the master and deploying
it to the workers.

\subsection{RDMA Distributed Key-Value Store}

Modern network technology allows access to the memory of remote machines
through the network cards (NICs) without involvement from the remote host
altogether~\cite{Hamada_infinibandtrade,Beck:2011:PER:2043535.2043537}.
This has opened up the development of
extremely efficient remote memory services, often in the form of DKV stores.
This project stores $\pi$ in a DKV store in the collective memory of the
cluster, where $\pi[i]+\pi^{sum}[i]$ is the value for key $i$. We decided not
to use any of the existing implementations of RDMA DKV stores, and instead
build our own DKV store on top of the ib-verbs API~\cite{ib-verbs}. The
main consideration is that our use case is unusually simple in a number
of important aspects. The KV layout is static in the sense that there are
no inserts or deletes after the initial population, which allows a static
partitioning of KV pairs over the machines. The values are all of the same size,
a vector of $K+1$ floats. The access pattern is well controlled because the
computation is partitioned into stages that are separated by barriers. A
stage either reads values or updates values. The algorithm ensures that updates
are always targeting unique elements,
therefore, there are no read/write or write/write hazards. The existing DKV stores incur
overhead for insert/delete flexibility, concurrency control, or variable-sized
values. Our use case allows us to do any operation in exactly one RDMA read
or RDMA write. Herd~\cite{Kalia:2014:URE:2740070.2626299} argues
that replacing RDMA reads by
an RPC to the server, followed by an unreliable write, is faster. However,
they show this only holds for small payload packets, up to 256B. A $\pi$
packet in our application is typically thousands to hundreds of thousands
of 4-byte floats.

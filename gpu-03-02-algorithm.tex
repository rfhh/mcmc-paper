\begin{comment}
Stochastic: iteratively:
 - sample a mini-batch $M$ (from the graph, or not from G)
 - for each node in the mini-batch:
   sample $n$ random 'neighbors' (not from G)
   for each edge (node, neighbor), calculate its contribution to the gradient
   add noise to the gradient contributions to be able to jump out of local minima
 - sum the gradients for each node in the mini-batch
 - using the gradients, infer new value of $\phi$ for each node in the mini-batch
 - update $\pi$ accordingly
 - calculate the gradients for $\beta$ for the edges in the mini-batch, and
   update $\beta$ and $\theta$
 - every so many iterations:
    + verify the quality of $\pi$ and $\beta$ against the validation set; metric is
      perplexity
    + terminate when perplexity changes less than $\epsilon$
\end{comment}


% \begin{figure}[h]
\begin{lstlisting}[mathescape,caption=SG-MCMC MMSB algorithm,label=algorithm-pseudo-code]
iterate
   sample a mini-batch $M$
   // $M$ may be a subset of $G$, or disjoint with $G$
   for each vertex $i$ in $M$:
      sample $n$ random 'neighbors' (not from $G$)
      for each edge ($i$, neighbor)
         calculate its contribution to the gradient in $\phi$
      // to be able to jump out of local minima:
      add noise to the gradient contributions
      update $\phi[i]$ using stepsize $s$
   for each vertex $i$ in $M$:
      update $\pi[i]$ according to changed $\phi[i]$
   for the edges in $M$:
      calculate gradients in $\theta$ and update $\theta$
   for the edges in $M$:
      update $\beta$ accordingly
   every so many iterations:
      // metric is perplexity:
      verify the quality of $\pi$ and $\beta$ against the validation set
until perplexity change is less than $\epsilon$
\end{lstlisting}
% \end{figure}

\section{Conclusions}
\label{sec-conclusion}

Modern machine learning algorithms are empowering their
users to solve or approximate solutions to previously intractable problems.
However, these advancements come at the cost of significant computational
complexity and long running learning processes which hinder our ability to reap
such algorithms' benefits when applied to large problems. There is a clear need
to assess such algorithms and identify optimization opportunities in order to
scale their performance.

The SG-MCMC algorithm discussed in this paper posed additional computational
challenges due to its unique stochastic nature and high data intensity. Unlike
common machine learning algorithms, its data dependencies and memory access
patterns are nondeterministic. In this paper, we presented our methodology of
improving the algorithm's performance by fundamentally restructuring it to
cater for concurrency.

We showed that the algorithm's state can be reduced by 75\% after a thorough
analysis of the computational patterns and data structures which significantly
reduces its data intensity.

In our optimization for many-core processors, we navigated the complex optimization
landscape by dynamically generating kernel code and testing different
combinations of optimizations. The outcome of these efforts culminated in
significant speedup factors of 21 and 86 using a multi-core CPU and a GPU
respectively. These speedup numbers were achieved in comparison to an already
optimized sequential implementation. Finally, we evaluated the performance of
the parallel algorithm across several GPUs, highlighting the difference between
their optimal configurations.
%
The outcome of this study reinforces the significance of avoiding premature
optimization as it can lead to unexpected results. In particular, the success
of common GPU optimizations depends on the particular device in use and the
problem it is applied to.

We further created a highly scalable implementation to handle the largest existing
community graphs.
This design faced
several obstacles in order to achieve high performance.
The algorithm was restructured to facilitate its
parallelization. We overlapped computation
with communication to hide latency.  We use a
mixture of MPI and RDMA primitives to speedup the communication between cluster
nodes.
%
Further, we conducted a thorough empirical evaluation of the system to study its strong
and weak scalability on 65 cluster nodes using large data sets.
% Additionally, we assessed the efficiency of the algorithm's resource utilization.
Finally, a
demonstration of the implementation's utility was provided by processing 6
different organic data sets.
To the best of
our knowledge, this is the first time that the problem of deducing overlapping
communities has been learned for problems of such a large scale.

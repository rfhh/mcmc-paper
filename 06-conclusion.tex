\section{Conclusions}
\label{sec-conclusion}

Modern machine learning algorithms are empowering their
users to solve or approximate solutions to previously intractable problems.
However, these advancements come at the cost of significant computational
complexity and long running learning processes which hinder our ability to reap
such algorithms' benefits when applied to large problems. There is a clear need
to assess such algorithms and identify optimization opportunities in order to
scale their performance.

The SG-MCMC algorithm discussed in this paper posed additional computational
challenges due to its unique stochastic nature and \emph{high data intensity}. Unlike
common machine learning algorithms, its data dependencies and memory access
patterns are \emph{nondeterministic}. In this paper, we presented our methodology of
improving the algorithm's performance by restructuring it to
cater for concurrent and distributed parallelism.

We showed that the algorithm's state can be reduced by 75\% after a thorough
analysis of the computational patterns and data structures, thus significantly
reducing its data intensity.

In our implementation for many-core processors, we navigated the complex optimization
landscape by dynamically generating kernel code and investigating different
combinations of optimizations. The outcome of these efforts culminated in
significant speedup factors of 21 and 120 using a multi-core CPU and a GPU
respectively. These speedup numbers were achieved in comparison to an already
optimized sequential implementation. The evaluation of the performance
across several GPUs yielded one striking find. No single optimization strategy
works best for all GPUs: different choices for memory allocation strategy,
kernel block size and vector width were required for optimal results for
different GPUs and application instances.
%
Hence, the outcome of this study reinforces the significance of avoiding premature
optimization. In particular, the success
of common GPU optimizations depends on the device and the
problem size it is applied to.

We further created a highly scalable implementation to handle the largest existing
community graphs on a distributed cluster machine.
This design solved
several problems to achieve scalability and high performance.
We overlapped computation
with communication to hide latency.  We use a
mixture of MPI and RDMA primitives to speed up the communication between cluster
nodes.
%
Further, we conducted a thorough empirical evaluation of the system to study its strong
and weak scalability on 65 cluster nodes using large data sets.
% Additionally, we assessed the efficiency of the algorithm's resource utilization.
Finally, a
demonstration of the implementation's utility was provided by processing 6
large real-world data sets.
To the best of
our knowledge, this is the first time that the problem of deducing overlapping
communities has been learned for problems of such a large scale.
